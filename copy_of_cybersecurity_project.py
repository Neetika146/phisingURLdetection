# -*- coding: utf-8 -*-
"""Copy of Cybersecurity Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V6s0L2LObdNioHbnfTH95i68Fa_sbHH2

# **CYBERSECURITY PROJECT**

#**Phishing URL Detection**

###**Objective of Malicious URL Detection**
The importance to safeguard online users from becoming victims of online fraud, divulging confidential information to an attacker among other effective uses of phishing as an attacker's tool, phishing detection tools play a vital role in ensuring a secure online experience for users.

Group 22
(IT -1)
*   Shreya Singhal (02701032021)
*   Neetika Tandon (04001032021)
*   Arnisha Barman (03401032021)
*   Shreya Ganjoo (02201032021)
*   Sanya Gupta (07401032021)

#**1.** **Installing libraries and packages**
"""

!pip install selenium

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd # for data manipulation and analysis
import numpy as np # for multi-dimensional array and matrix

import seaborn as sns # for high-level interface for drawing attractive and informative statistical graphics 
import matplotlib.pyplot as plt # data visualization
# %matplotlib inline 
# It sets the backend of matplotlib to the 'inline' backend:
import plotly.express as px
import time # calculate time 

from sklearn.linear_model import LogisticRegression # algo use to predict good or bad
from sklearn.naive_bayes import MultinomialNB # nlp algo use to predict good or bad

from sklearn.model_selection import train_test_split # spliting the data between feature and target
from sklearn.metrics import classification_report # gives whole report about metrics (e.g, recall,precision,f1_score,c_m)
from sklearn.metrics import confusion_matrix # gives info about actual and predict
from nltk.tokenize import RegexpTokenizer # regexp tokenizers use to split words from text  
from nltk.stem.snowball import SnowballStemmer # stemmes words
from sklearn.feature_extraction.text import CountVectorizer # converts words into numerical data   
from sklearn.pipeline import make_pipeline # use for combining all prerocessors techniques and algos

from PIL import Image # getting images in notebook
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# creates words colud

from bs4 import BeautifulSoup # use for scraping the data from website
from selenium import webdriver # use for automation chrome 
import networkx as nx # for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.

import pickle# use to dump model 

import warnings # ignores pink warnings 
warnings.filterwarnings('ignore')

"""###**1.1 Exploratory Data Analysis(EDA)**"""

phish_data = pd.read_csv('/content/phishing_site_urls.csv')
phish_data

phish_data.head() # first 5 data

phish_data.tail() # last 5 data

phish_data.shape # display row and col no

phish_data.info() # non-null value checking

phish_data.describe() # aggregate function

phish_data.nunique() # checking unique values

phish_data.isnull().sum() # there is no missing values

#create a dataframe of classes counts
label_counts = pd.DataFrame(phish_data.Label.value_counts())

#visualizing target_col
fig = px.bar(label_counts,x=label_counts.index, y=label_counts.Label)
fig.show()

"""According to the above plot, more than 3,50,000 URLs are good i.e legitimate while the rest are phishing URLs.

#**2. Preprocessing**

**2.1 RegexpTokenizer**

A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.
"""

tokenizer = RegexpTokenizer(r'[A-Za-z]+')#to getting alpha only

phish_data.URL[0]

# this will be pull letter which matches to expression
tokenizer.tokenize(phish_data.URL[0]) # using first row

print('Getting words tokenized ...')
t0= time.perf_counter()
phish_data['text_tokenized'] = phish_data.URL.map(lambda t: tokenizer.tokenize(t)) # doing with all rows
t1 = time.perf_counter() - t0
print('Time taken',t1 ,'sec')

phish_data.sample(5) #random data

"""**2.2 SnowballStemmer**

Snowball is a small string processing language, gives root words.
"""

stemmer = SnowballStemmer("english") # choose a language

print('Getting words stemmed ...')
t0= time.perf_counter()
phish_data['text_stemmed'] = phish_data['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])
t1= time.perf_counter() - t0
print('Time taken',t1 ,'sec')

phish_data.sample(5)

print('Getting joiningwords ...')
t0= time.perf_counter()
phish_data['text_sent'] = phish_data['text_stemmed'].map(lambda l: ' '.join(l))
t1= time.perf_counter() - t0
print('Time taken',t1 ,'sec')

phish_data.sample(5)

"""#**3. Visualization**

Visualize some important keys using word cloud
"""

#sliceing classes
bad_sites = phish_data[phish_data.Label == 'bad']
good_sites = phish_data[phish_data.Label == 'good']

bad_sites.head()

good_sites.head()

"""**create a function to visualize the important keys from url**"""

def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), 
                   title = None, title_size=40, image_color=False):
    stopwords = set(STOPWORDS)
    more_stopwords = {'com','http'}
    stopwords = stopwords.union(more_stopwords)

    wordcloud = WordCloud(background_color='white',
                    stopwords = stopwords,
                    max_words = max_words,
                    max_font_size = max_font_size, 
                    random_state = 42,
                    mask = mask)
    wordcloud.generate(text)
    
    plt.figure(figsize=figure_size)
    if image_color:
        image_colors = ImageColorGenerator(mask);
        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation="bilinear");
        plt.title(title, fontdict={'size': title_size,  
                                  'verticalalignment': 'bottom'})
    else:
        plt.imshow(wordcloud);
        plt.title(title, fontdict={'size': title_size, 'color': 'green', 
                                  'verticalalignment': 'bottom'})
    plt.axis('off');
    plt.tight_layout()  
d = '../input/masks/masks-wordclouds/'

data = good_sites.text_sent
data.reset_index(drop=True, inplace=True)

data = bad_sites.text_sent
data.reset_index(drop=True, inplace=True)

"""#**4. Creating Model**
###**4.1 CountVectorizer**

CountVectorizer is used to transform a corpora of text to a vector of term / token counts.
"""

#create cv object
cv = CountVectorizer()

help(CountVectorizer())

feature = cv.fit_transform(phish_data.text_sent) #transform all text which we tokenize and stemmed

feature[:5].toarray() # convert sparse matrix into array to print transformed features

"""**4.2 Spliting the data**"""

trainX, testX, trainY, testY = train_test_split(feature, phish_data.Label)

"""# **5. LogisticRegression**

Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.
"""

# create lr object
lr = LogisticRegression()

lr.fit(trainX,trainY)

lr.score(testX,testY)

"""**Logistic Regression is giving 96.4% accuracy, Now we will store scores in dict to see which model perform best.**"""

Scores_ml = {}

Scores_ml['Logistic Regression'] = np.round(lr.score(testX,testY),2)
print('Training Accuracy :',lr.score(trainX,trainY))
print('Testing Accuracy :',lr.score(testX,testY))
con_mat = pd.DataFrame(confusion_matrix(lr.predict(testX), testY),
            columns = ['Predicted:Bad', 'Predicted:Good'],
            index = ['Actual:Bad', 'Actual:Good'])

print('\nCLASSIFICATION REPORT\n')
print(classification_report(lr.predict(testX), testY,
                            target_names =['Bad','Good']))

print('\nCONFUSION MATRIX')
plt.figure(figsize= (6,4))
sns.heatmap(con_mat, annot = True,fmt='d',cmap="YlGnBu")

# Precision quantifies the number of positive class predictions that actually belong to the positive class.
# Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.
# F-Measure provides a single score that balances both the concerns of precision and recall in one number.

"""# **6. Multinomial Naive Bayes**

Applying Multinomial Naive Bayes to NLP Problems. Naive Bayes Classifier Algorithm is a family of probabilistic algorithms based on applying Bayes' theorem with the “naive” assumption of conditional independence between every pair of a feature.
"""

# create mnb object
mnb = MultinomialNB()

mnb.fit(trainX,trainY)

mnb.score(testX,testY)

"""**Multinomial Naive Bayes gives us 95% accuracy.**"""

Scores_ml['MultinomialNB'] = np.round(mnb.score(testX,testY),2)

print('Training Accuracy :',mnb.score(trainX,trainY))
print('Testing Accuracy :',mnb.score(testX,testY))
con_mat = pd.DataFrame(confusion_matrix(mnb.predict(testX), testY),
            columns = ['Predicted:Bad', 'Predicted:Good'],
            index = ['Actual:Bad', 'Actual:Good'])


print('\nCLASSIFICATION REPORT\n')
print(classification_report(mnb.predict(testX), testY,
                            target_names =['Bad','Good']))

print('\nCONFUSION MATRIX')
plt.figure(figsize= (6,4))
sns.heatmap(con_mat, annot = True,fmt='d',cmap="YlGnBu")

acc = pd.DataFrame.from_dict(Scores_ml,orient = 'index',columns=['Accuracy'])
sns.set_style('whitegrid')
sns.barplot(acc.index,acc.Accuracy)
# Logistic Regression is giving 96.4% accuracy
# Multinomial Naive Bayes gives us 95% accuracy.

"""**So, Logistic Regression is the best fit model as shown above. Now we make sklearn pipeline using Logistic Regression**"""

pipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression())
##(r'\b(?:http|ftp)s?://\S*\w|\w+|[^\w\s]+') ([a-zA-Z]+)([0-9]+)  -- these tolenizers giving me low accuray

trainX, testX, trainY, testY = train_test_split(phish_data.URL, phish_data.Label)

pipeline_ls.fit(trainX,trainY)

pipeline_ls.score(testX,testY)

print('Training Accuracy :',pipeline_ls.score(trainX,trainY))
print('Testing Accuracy :',pipeline_ls.score(testX,testY))
con_mat = pd.DataFrame(confusion_matrix(pipeline_ls.predict(testX), testY),
            columns = ['Predicted:Bad', 'Predicted:Good'],
            index = ['Actual:Bad', 'Actual:Good'])


print('\nCLASSIFICATION REPORT\n')
print(classification_report(pipeline_ls.predict(testX), testY,
                            target_names =['Bad','Good']))

print('Training Accuracy :',pipeline_ls.score(trainX,trainY))
print('Testing Accuracy :',pipeline_ls.score(testX,testY))
con_mat = pd.DataFrame(confusion_matrix(pipeline_ls.predict(testX), testY),
            columns = ['Predicted:Bad', 'Predicted:Good'],
            index = ['Actual:Bad', 'Actual:Good'])


print('\nCLASSIFICATION REPORT\n')
print(classification_report(pipeline_ls.predict(testX), testY,
                            target_names =['Bad','Good']))

print('\nCONFUSION MATRIX')
plt.figure(figsize= (6,4))
sns.heatmap(con_mat, annot = True,fmt='d',cmap="YlGnBu")

pickle.dump(pipeline_ls,open('phishing.pkl','wb'))

loaded_model = pickle.load(open('phishing.pkl', 'rb'))
result = loaded_model.score(testX,testY)
print(result)

"""See, it's that simple yet so effective. We get an accuracy of 96%. That’s a very high value for a machine to be able to detect a malicious URL with. Want to test some links to see if the model gives good predictions? Sure. Let's do it

**Bad links** => this are phishing sites yeniik.com.tr/wp-admin/js/login.alibaba.com/login.jsp.php fazan-pacir.rs/temp/libraries/ipad www.tubemoviez.exe svision-online.de/mgfi/administrator/components/com_babackup/classes/fx29id1.txt

**Good links** => this are not phishing sites www.youtube.com/ youtube.com/watch?v=qI0TQJI3vdU www.retailhellunderground.com/ restorevisioncenters.com/html/technology.html
"""

predict_bad = ['www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrcmd=_home-customer&nav=1/loading.php','smilesvoegol.servebbs.org/voegol.php','tubemoviez.exe','super1000.info/docs','www.coincoele.com.br/Scripts/smiles/?pt-br/Paginas/default.aspx','perfectsolutionofall.net/wp-content/themes/twentyten/wiresource/']
predict_good = ['www.financialaccounting.com/financialprof.htm','youtube.com/watch?v=qI0TQJI3vdU','zyra.org.uk/greenandblacks.htm','restorevisioncenters.com/html/technology.html']
loaded_model = pickle.load(open('phishing.pkl', 'rb'))
#predict_bad = vectorizers.transform(predict_bad)
# predict_good = vectorizer.transform(predict_good)
result = loaded_model.predict(predict_bad)
result2 = loaded_model.predict(predict_good)
print(result)
print("*"*30)
print(result2)

"""The above model predicts the URLS as bad or good i.e Phishing or Legitimate with an accuracy of 96% using Logistic Regression.

"""